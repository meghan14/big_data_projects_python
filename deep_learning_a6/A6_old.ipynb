{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 6 - Recognizing objects in images with deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will get to know the main ingredients of deep learning and get started using the GPUs available in the Big Data Lab.\n",
    "\n",
    "You'll learn to use\n",
    "\n",
    " * tensors\n",
    " * automatic differentiation\n",
    " * layered learners\n",
    " * p(re)trained networks for image classification.\n",
    "\n",
    "## Check the GPU setup\n",
    "\n",
    "When you are logged in to a lab machine, run ``nvidia-smi`` to see the available card and its memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ nvidia-smi\n",
    "Mon Feb  5 08:03:15 2018       \n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 390.12                 Driver Version: 390.12                    |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  GeForce GTX 105...  Off  | 00000000:01:00.0  On |                  N/A |\n",
    "| 45%   24C    P8    N/A /  75W |   3087MiB /  4038MiB |      0%      Default |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "                                                                               \n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                       GPU Memory |\n",
    "|  GPU       PID   Type   Process name                             Usage      |\n",
    "|=============================================================================|\n",
    "|    0      3627      G   /usr/lib/xorg/Xorg                           169MiB |\n",
    "|    0     10843      C   ...d/CMPT/big-data/tmp_py/dlenv/bin/python  2897MiB |\n",
    "+-----------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that the machine has an **NVIDIA GTX 1050 with 4G of RAM**. Also, you can see that I'm running a process (pid=10843) that currently takes up close to 3 GB of GPU memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "$ pstree -ls 10843\n",
    "screen───bash───jupyter-noteboo───python─┬─4*[python]\n",
    "                                         └─26*[{python}]\n",
    "```\n",
    "Inside a terminal window you may use ``who``, ``ps -aux | less``, or ``pstree -ls <PID>`` as above to find out who is using the shared resources. In my case, it turns out that I'm running a jupyter notebook related to process 10843. Halting the notebook frees up the GPU memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch setup in the lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we are going to use **[pytorch](http://pytorch.org)**, which received some praise recently for being faster than [tensorflow](http://tensorflow.org) and for also having a nice high-level API as NN modules that are similar to [Keras](https://keras.io/).\n",
    "\n",
    "The default `conda` environment has pytorch 1.0 installed. This means, you should be able to use it without any changes to your environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use our downloaded pre-built models\n",
    "To save disk space in your home folder, we recommend that you let pytorch use the pre-built models that we already downloaded for you (about 1.6G):\n",
    "```\n",
    "mkdir -p ~/.torch/models\n",
    "ln -s /usr/shared/CMPT/big-data/dot_torch_shared/models/models/* ~/.torch/models\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn about Pytorch usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To familiarize yourself with PyTorch, have a look at the [Examples](http://pytorch.org/tutorials/beginner/pytorch_with_examples.html) or briefly skim over the [60 min blitz tutorial](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Finding rectangles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice blog-post by [Johannes Rieke](https://towardsdatascience.com/object-detection-with-neural-networks-a4e2c46b4491) presents a simple setup from scratch that finds rectangles in a black & white image. In order to play with it, we just have to translate a few calls from Keras to PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Here is an example of the training data:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Create images with random rectangles and bounding boxes. \n",
    "num_imgs = 50000\n",
    "\n",
    "img_size = 8\n",
    "min_object_size = 1\n",
    "max_object_size = 4\n",
    "num_objects = 1\n",
    "\n",
    "bboxes = np.zeros((num_imgs, num_objects, 4))\n",
    "imgs = np.zeros((num_imgs, img_size, img_size))  # set background to 0\n",
    "\n",
    "for i_img in range(num_imgs):\n",
    "    for i_object in range(num_objects):\n",
    "        w, h = np.random.randint(min_object_size, max_object_size, size=2)\n",
    "        x = np.random.randint(0, img_size - w)\n",
    "        y = np.random.randint(0, img_size - h)\n",
    "        imgs[i_img, x:x+w, y:y+h] = 1.  # set rectangle to 1\n",
    "        bboxes[i_img, i_object] = [x, y, w, h]\n",
    "        \n",
    "imgs.shape, bboxes.shape\n",
    "\n",
    "display(Markdown('**Here is an example of the training data:**'))\n",
    "i = 0\n",
    "plt.imshow(imgs[i].T, cmap='Greys', interpolation='none', origin='lower', extent=[0, img_size, 0, img_size])\n",
    "for bbox in bboxes[i]:\n",
    "    plt.gca().add_patch(matplotlib.patches.Rectangle((bbox[0], bbox[1]), bbox[2], bbox[3], ec='r', fc='none'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 64) 1.1452616632823265e-16 0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "# Reshape and normalize the image data to mean 0 and std 1. \n",
    "X = (imgs.reshape(num_imgs, -1) - np.mean(imgs)) / np.std(imgs)\n",
    "print(X.shape, np.mean(X), np.std(X))\n",
    "\n",
    "# Normalize x, y, w, h by img_size, so that all values are between 0 and 1.\n",
    "# Important: Do not shift to negative values (e.g. by setting to mean 0), because the IOU calculation needs positive w and h.\n",
    "y = bboxes.reshape(num_imgs, -1) / img_size\n",
    "y.shape, np.mean(y), np.std(y)\n",
    "\n",
    "# Split training and test.\n",
    "i = int(0.8 * num_imgs)\n",
    "train_X = X[:i]\n",
    "test_X = X[i:]\n",
    "train_y = y[:i]\n",
    "test_y = y[i:]\n",
    "test_imgs = imgs[i:]\n",
    "test_bboxes = bboxes[i:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1a\n",
    "Construct a Pytorch model that resembles the Keras one in the original blog post, i.e. have a fully connected, hidden layer with 200 neurons, ReLU nonlinearity and dropout rate of 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(X.shape[-1],200), \n",
    "              nn.ReLU(),\n",
    "              nn.Linear(200,y.shape[-1]),\n",
    "              nn.ReLU(),\n",
    "              nn.Dropout(0.2),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adadelta(model.parameters())\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Variable(torch.Tensor(train_X))\n",
    "labels = Variable(torch.Tensor(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-b9e90a032fd8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mloss_record\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number"
     ]
    }
   ],
   "source": [
    "phase = 'train'\n",
    "running_loss = 0.0\n",
    "running_corrects = 0\n",
    "\n",
    "loss_record = []\n",
    "for epoch in range(30):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    loss = loss_fn(outputs, labels)\n",
    "\n",
    "    if phase == 'train':\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    running_loss += loss.data[0] * inputs.size(0)\n",
    "    epoch_loss = running_loss / inputs.shape[0] / (epoch+1)\n",
    "    loss_record.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1db36f4d9b0>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1, 4)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict bounding boxes on the test images.\n",
    "pred_y = model(Variable(torch.Tensor(test_X)))\n",
    "pred_bboxes = pred_y.data * img_size\n",
    "pred_bboxes = pred_bboxes.numpy().reshape(len(pred_bboxes), num_objects, -1)\n",
    "pred_bboxes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IOU(bbox1, bbox2):\n",
    "    '''Calculate overlap between two bounding boxes [x, y, w, h] as the area of intersection over the area of unity'''\n",
    "    x1, y1, w1, h1 = bbox1[0], bbox1[1], bbox1[2], bbox1[3]\n",
    "    x2, y2, w2, h2 = bbox2[0], bbox2[1], bbox2[2], bbox2[3]\n",
    "\n",
    "    w_I = min(x1 + w1, x2 + w2) - max(x1, x2)\n",
    "    h_I = min(y1 + h1, y2 + h2) - max(y1, y2)\n",
    "    if w_I <= 0 or h_I <= 0:  # no overlap\n",
    "        return 0.\n",
    "    I = w_I * h_I\n",
    "    U = w1 * h1 + w2 * h2 - I\n",
    "    return I / U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31191300262456884"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fbf202374e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show a few images and predicted bounding boxes from the test dataset. \n",
    "plt.figure(figsize=(12, 3))\n",
    "for i_subplot in range(1, 5):\n",
    "    plt.subplot(1, 4, i_subplot)\n",
    "    i = np.random.randint(len(test_imgs))\n",
    "    plt.imshow(test_imgs[i].T, cmap='Greys', interpolation='none', origin='lower', extent=[0, img_size, 0, img_size])\n",
    "    for pred_bbox, exp_bbox in zip(pred_bboxes[i], test_bboxes[i]):\n",
    "        plt.gca().add_patch(matplotlib.patches.Rectangle((pred_bbox[0], pred_bbox[1]), pred_bbox[2], pred_bbox[3], ec='r', fc='none'))\n",
    "        plt.annotate('IOU: {:.2f}'.format(IOU(pred_bbox, exp_bbox)), (pred_bbox[0], pred_bbox[1]+pred_bbox[3]+0.2), color='r')\n",
    "# Calculate the mean IOU (overlap) between the predicted and expected bounding boxes on the test dataset. \n",
    "summed_IOU = 0.\n",
    "for pred_bbox, test_bbox in zip(pred_bboxes.reshape(-1, 4), test_bboxes.reshape(-1, 4)):\n",
    "    summed_IOU += IOU(pred_bbox, test_bbox)\n",
    "mean_IOU = summed_IOU / len(pred_bboxes)\n",
    "mean_IOU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1b:\n",
    "Move the computation that is currently done on the CPU over to the GPU using CUDA and increase the number of epochs. Improve the training setup until you reach an IOU of above 0.6.\n",
    "You can make the changes that move computation to the GPU directly in the cells above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Use a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in class deep learning systems are hardly ever developed from scratch, but usually work by refining existing solutions to similar problems. For the following task, we'll work through the \n",
    "[Transfer learning tutorial](http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html), which also provides a ready-made jupyter notebook.\n",
    "\n",
    " 1. Download the notebook and get it to run in your environment. This also involves downloading the bees and ants dataset.\n",
    " 2. Perform your own training with the provided setup.Copy the relevant code into this notebook, as mentioned in the submission instructions.\n",
    " 3. Change the currently chosen pretrained network (resnet) to a different one. At least try out VGG and one other type.\n",
    " 4. Load a picture that you took yourself and classify it with an unmodified pretrained network (e.g. the original VGG network) that can detect one out of 1000 classes. Display the image and class label in the notebook for submission.\n",
    "\n",
    "### Hints for step 3\n",
    "\n",
    "Focus on the section **Conv net as fixed feature xtractor** of the transfer learning tutorial.\n",
    "First, change the line\n",
    "```\n",
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "```\n",
    "to load VGG19 instead.\n",
    "\n",
    "Next, print out the new model_conv and identify the last step of the classification. This is not named the same way as the ```fc``` layer for resnet, but works similarily. Identify the module that contains the last classification step of the VGG model, which identifies one out of 1000 classes. Change that one into identifying 2 classes only (i.e. the ants and bees that you should start with).\n",
    "\n",
    "To change the structure of Sequential component called ```module_name``` and to modify its last layer into a DifferentLayer type, you can use this syntax.\n",
    "\n",
    "```\n",
    "nn.Sequential(*list(model_conv.module_name.children())[:-1] +\n",
    "                     [nn.DifferentLayer(...)])\n",
    "```\n",
    "and replace the old model_conv.module_name with the differently structured version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For task 1 add your changes to the cells in this notebook.\n",
    "\n",
    "For task 2 append the required fields of the the transfer learning tutorial to this notebook and make the required modifications. Ensure that you have clear section headings that show where you undertake which parts of the question.\n",
    "\n",
    "Save this notebook containing all images and upload your submission as one [A6.ipynb](A6.ipynb) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
