{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_resolution.py\n",
    "import re\n",
    "import operator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark import SparkContext\n",
    "\n",
    "\n",
    "class EntityResolution:\n",
    "    def __init__(self, dataFile1, dataFile2, stopWordsFile):\n",
    "        spark = SparkSession.builder.appName(\"ER example\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "        sc = spark.sparkContext\n",
    "        self.f = open(stopWordsFile, \"r\")\n",
    "        self.stopWords = set(self.f.read().split(\"\\n\"))\n",
    "        self.stopWordsBC = sc.broadcast(self.stopWords).value\n",
    "        self.df1 = spark.read.parquet(dataFile1).cache()\n",
    "        self.df2 = spark.read.parquet(dataFile2).cache()\n",
    "\n",
    "    def preprocessDF(self, df, cols): \n",
    "        \"\"\"\n",
    "        Input: $df represents a DataFrame\n",
    "               $cols represents the list of columns (in $df) that will be concatenated and be tokenized\n",
    "\n",
    "        Output: Return a new DataFrame that adds the \"joinKey\" column into the input $df\n",
    "\n",
    "        Comments: The \"joinKey\" column is a list of tokens, which is generated as follows:\n",
    "                 (1) concatenate the $cols in $df; \n",
    "                 (2) apply the tokenizer to the concatenated string\n",
    "        Here is how the tokenizer should work:\n",
    "                 (1) Use \"re.split(r'\\W+', string)\" to split a string into a set of tokens\n",
    "                 (2) Convert each token to its lower-case\n",
    "                 (3) Remove stop words\n",
    "        \"\"\"\n",
    "        df = df.withColumn(\"joinkeys\",func_udf(df.select(cols),self.stopWords))\n",
    "        df.show()\n",
    "\n",
    "    def filtering(self, df1, df2):\n",
    "        \"\"\" \n",
    "        Input: $df1 and $df2 are two input DataFrames, where each of them \n",
    "               has a 'joinKey' column added by the preprocessDF function\n",
    "\n",
    "        Output: Return a new DataFrame $candDF with four columns: 'id1', 'joinKey1', 'id2', 'joinKey2',\n",
    "                where 'id1' and 'joinKey1' are from $df1, and 'id2' and 'joinKey2'are from $df2.\n",
    "                Intuitively, $candDF is the joined result between $df1 and $df2 on the condition that \n",
    "                their joinKeys share at least one token. \n",
    "\n",
    "        Comments: Since the goal of the \"filtering\" function is to avoid n^2 pair comparisons, \n",
    "                  you are NOT allowed to compute a cartesian join between $df1 and $df2 in the function. \n",
    "                  Please come up with a more efficient algorithm (see hints in Lecture 2). \n",
    "        \"\"\"\n",
    "\n",
    "    def verification(self, candDF, threshold):\n",
    "        \"\"\"\n",
    "             Input: $candDF is the output DataFrame from the 'filtering' function. \n",
    "                   $threshold is a float value between (0, 1] \n",
    "\n",
    "            Output: Return a new DataFrame $resultDF that represents the ER result. \n",
    "                    It has five columns: id1, joinKey1, id2, joinKey2, jaccard \n",
    "\n",
    "            Comments: There are two differences between $candDF and $resultDF\n",
    "                      (1) $resultDF adds a new column, called jaccard, which stores the jaccard similarity \n",
    "                          between $joinKey1 and $joinKey2\n",
    "                      (2) $resultDF removes the rows whose jaccard similarity is smaller than $threshold\n",
    "        \"\"\"\n",
    "\n",
    "    def evaluate(self, result, groundTruth):\n",
    "        \"\"\"\n",
    "        Input: $result is a list of matching pairs identified by the ER algorithm\n",
    "               $groundTrueth is a list of matching pairs labeld by humans\n",
    "\n",
    "        Output: Compute precision, recall, and fmeasure of $result based on $groundTruth, and\n",
    "                return the evaluation result as a triple: (precision, recall, fmeasure)\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    def jaccardJoin(self, cols1, cols2, threshold):\n",
    "        newDF1 = self.preprocessDF(self.df1, cols1)\n",
    "        newDF2 = self.preprocessDF(self.df2, cols2)\n",
    "        print (\"Before filtering: %d pairs in total\" %(self.df1.count()*self.df2.count())) \n",
    "\n",
    "        candDF = self.filtering(newDF1, newDF2)\n",
    "        print (\"After Filtering: %d pairs left\" %(candDF.count()))\n",
    "\n",
    "        resultDF = self.verification(candDF, threshold)\n",
    "        print (\"After Verification: %d similar pairs\" %(resultDF.count()))\n",
    "\n",
    "        return resultDF\n",
    "\n",
    "\n",
    "    def __del__(self):\n",
    "        self.f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(list_elements,stopwrds):\n",
    "    new_string = \" \".join(list_elements)\n",
    "    tokens = re.split(\"\\rW+\",new_string)\n",
    "    return list(set(tokens)-set(stopwrds))\n",
    "func_udf = udf(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid argument, not a string or column: DataFrame[title: string, manufacturer: string] of type <class 'pyspark.sql.dataframe.DataFrame'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-9b8f4aa542ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mamazonCols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"title\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"manufacturer\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mgoogleCols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"name\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"manufacturer\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mresultDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjaccardJoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamazonCols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgoogleCols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresultDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-c395ae272fa0>\u001b[0m in \u001b[0;36mjaccardJoin\u001b[1;34m(self, cols1, cols2, threshold)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mjaccardJoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mnewDF1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[0mnewDF2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessDF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Before filtering: %d pairs in total\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-35-c395ae272fa0>\u001b[0m in \u001b[0;36mpreprocessDF\u001b[1;34m(self, df, cols)\u001b[0m\n\u001b[0;32m     32\u001b[0m                  \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[0mRemove\u001b[0m \u001b[0mstop\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \"\"\"\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"joinkeys\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfunc_udf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopWords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\udf.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;33m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0massigned\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0massignments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\udf.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mjudf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_judf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjudf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# This function is for improving the online help system in the interactive interpreter.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\column.py\u001b[0m in \u001b[0;36m_to_seq\u001b[1;34m(sc, cols, converter)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \"\"\"\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\column.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \"\"\"\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mcols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[1;34m(col)\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;34m\"{0} of type {1}. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;34m\"For column literals, use 'lit', 'array', 'struct' or 'create_map' \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \"function.\".format(col, type(col)))\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mjcol\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid argument, not a string or column: DataFrame[title: string, manufacturer: string] of type <class 'pyspark.sql.dataframe.DataFrame'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function EntityResolution.__del__ at 0x000001F8225CE598>\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-21-f4d1c0977845>\", line 54, in __del__\n",
      "AttributeError: 'EntityResolution' object has no attribute 'f'\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    source_path = \"data/amazon-google-sample/\"\n",
    "    er = EntityResolution(source_path+\"Amazon_sample\", source_path+\"Google_sample\", source_path+\"stopwords.txt\")\n",
    "    amazonCols = [\"title\", \"manufacturer\"]\n",
    "    googleCols = [\"name\", \"manufacturer\"]\n",
    "    resultDF = er.jaccardJoin(amazonCols, googleCols, 0.5)\n",
    "\n",
    "    result = resultDF.map(lambda row: (row.id1, row.id2)).collect()\n",
    "    groundTruth = spark.read.parquet(\"Amazon_Google_perfectMapping_sample\") \\\n",
    "                          .map(lambda row: (row.idAmazon, row.idGoogle)).collect()\n",
    "    print (\"(precision, recall, fmeasure) = \", er.evaluate(result, groundTruth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
