{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entity_resolution.py\n",
    "import re\n",
    "import operator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, StringType,DoubleType\n",
    "from pyspark.sql.functions import udf,lower,col,concat_ws,split,explode,length\n",
    "from pyspark import SparkContext\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ER example\").config(\"spark.some.config.option\", \"value\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "class EntityResolution:\n",
    "    def __init__(self, dataFile1, dataFile2, stopWordsFile):\n",
    "        self.f = open(stopWordsFile, \"r\")\n",
    "        self.stopWords = set(self.f.read().split(\"\\n\"))\n",
    "        self.stopWordsBC = sc.broadcast(self.stopWords).value\n",
    "        self.df1 = spark.read.parquet(dataFile1).cache()\n",
    "        self.df2 = spark.read.parquet(dataFile2).cache()\n",
    "\n",
    "    def preprocessDF(self, df, cols): \n",
    "        \"\"\"\n",
    "        Input: $df represents a DataFrame\n",
    "               $cols represents the list of columns (in $df) that will be concatenated and be tokenized\n",
    "\n",
    "        Output: Return a new DataFrame that adds the \"joinKey\" column into the input $df\n",
    "\n",
    "        Comments: The \"joinKey\" column is a list of tokens, which is generated as follows:\n",
    "                 (1) concatenate the $cols in $df; \n",
    "                 (2) apply the tokenizer to the concatenated string\n",
    "        Here is how the tokenizer should work:\n",
    "                 (1) Use \"re.split(r'\\W+', string)\" to split a string into a set of tokens\n",
    "                 (2) Convert each token to its lower-case\n",
    "                 (3) Remove stop words\n",
    "        \"\"\"\n",
    "        def tokenize(words):\n",
    "            return [w for w in words if w not in list(stopwrds)]\n",
    "\n",
    "        stopwrds = self.stopWords\n",
    "        df = df.select(df.id, lower(concat_ws(' ', *cols)).alias('joinKey'))\n",
    "        df = df.select(df.id, split(df.joinKey, r'\\W+').alias('joinKey'))\n",
    "        #df = df.withColumn(\"joinkeys\",func_udf(df.select(cols),self.stopWords))\n",
    "        \n",
    "        filter_udf = udf(tokenize, ArrayType(StringType()))\n",
    "        df_filtered = df.select(df.id, filter_udf(df.joinKey).alias('joinKey'))\n",
    "        return(df_filtered)\n",
    "\n",
    "    def filtering(self, df1, df2):\n",
    "        \"\"\" \n",
    "        Input: $df1 and $df2 are two input DataFrames, where each of them \n",
    "               has a 'joinKey' column added by the preprocessDF function\n",
    "\n",
    "        Output: Return a new DataFrame $candDF with four columns: 'id1', 'joinKey1', 'id2', 'joinKey2',\n",
    "                where 'id1' and 'joinKey1' are from $df1, and 'id2' and 'joinKey2'are from $df2.\n",
    "                Intuitively, $candDF is the joined result between $df1 and $df2 on the condition that \n",
    "                their joinKeys share at least one token. \n",
    "\n",
    "        Comments: Since the goal of the \"filtering\" function is to avoid n^2 pair comparisons, \n",
    "                  you are NOT allowed to compute a cartesian join between $df1 and $df2 in the function. \n",
    "                  Please come up with a more efficient algorithm (see hints in Lecture 2). \n",
    "        \"\"\"\n",
    "        def flattenThisDF(df):\n",
    "            df = df.select(df.id, explode(col(\"joinKey\")).alias('joinKey')).cache()\n",
    "            df = df.filter(length(df.joinKey)>0)\n",
    "            return(df)\n",
    "\n",
    "        df1_flat = flattenThisDF(df1).cache()\n",
    "        df2_flat = flattenThisDF(df2).cache()\n",
    "\n",
    "        canDF = df1_flat.join(df2_flat, df1_flat.joinKey == df2_flat.joinKey).\\\n",
    "                                select(df1_flat.id.alias('id1'), df2_flat.id.alias('id2'))\n",
    "        canDF = canDF.drop_duplicates().cache()\n",
    "        canDF = canDF.join(df1, canDF.id1 == df1.id).\\\n",
    "                                select(canDF.id1, canDF.id2, df1.joinKey.alias('joinKey1')).cache()\n",
    "        canDF = canDF.join(df2, canDF.id2 == df2.id).\\\n",
    "                                select(canDF.id1, canDF.id2, canDF.joinKey1, df2.joinKey.alias('joinKey2'))\n",
    "        return (canDF)\n",
    "\n",
    "    def verification(self, candDF, threshold):\n",
    "        \"\"\"\n",
    "             Input: $candDF is the output DataFrame from the 'filtering' function. \n",
    "                   $threshold is a float value between (0, 1] \n",
    "\n",
    "            Output: Return a new DataFrame $resultDF that represents the ER result. \n",
    "                    It has five columns: id1, joinKey1, id2, joinKey2, jaccard \n",
    "\n",
    "            Comments: There are two differences between $candDF and $resultDF\n",
    "                      (1) $resultDF adds a new column, called jaccard, which stores the jaccard similarity \n",
    "                          between $joinKey1 and $joinKey2\n",
    "                      (2) $resultDF removes the rows whose jaccard similarity is smaller than $threshold\n",
    "        \"\"\"\n",
    "        def jaccard(key1, key2):\n",
    "            num = len(set(key1).intersection(set(key2)))\n",
    "            den = len(set(key1).union(set(key2)))\n",
    "            return float(num/den)\n",
    "\n",
    "        jaccard_udf = udf(jaccard, DoubleType())\n",
    "        df = candDF.select(candDF.id1, candDF.id2, jaccard_udf(candDF.joinKey1, candDF.joinKey2).alias('jaccard')).cache()\n",
    "        df = df.filter(df.jaccard >= threshold)\n",
    "        return(df)\n",
    "\n",
    "    def evaluate(self, result, groundTruth):\n",
    "        \"\"\"\n",
    "        Input: $result is a list of matching pairs identified by the ER algorithm\n",
    "               $groundTrueth is a list of matching pairs labeld by humans\n",
    "\n",
    "        Output: Compute precision, recall, and fmeasure of $result based on $groundTruth, and\n",
    "                return the evaluation result as a triple: (precision, recall, fmeasure)\n",
    "\n",
    "        \"\"\"\n",
    "        match = set(result).intersection(groundTruth)\n",
    "        precision = float(len(match)/len(result))\n",
    "        recall = float(len(match)/len(groundTruth))\n",
    "        FMeasure = float((2* precision * recall) / (precision + recall))\n",
    "        return (precision, recall, FMeasure)\n",
    "\n",
    "    def jaccardJoin(self, cols1, cols2, threshold):\n",
    "        newDF1 = self.preprocessDF(self.df1, cols1)\n",
    "        newDF2 = self.preprocessDF(self.df2, cols2)\n",
    "        print (\"Before filtering: %d pairs in total\" %(self.df1.count()*self.df2.count())) \n",
    "\n",
    "        candDF = self.filtering(newDF1, newDF2)\n",
    "        print (\"After Filtering: %d pairs left\" %(candDF.count()))\n",
    "\n",
    "        resultDF = self.verification(candDF, threshold)\n",
    "        print (\"After Verification: %d similar pairs\" %(resultDF.count()))\n",
    "\n",
    "        return resultDF\n",
    "\n",
    "\n",
    "    def __del__(self):\n",
    "        self.f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering: 40000 pairs in total\n",
      "After Filtering: 3947 pairs left\n",
      "After Verification: 82 similar pairs\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9b8f4aa542ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mresultDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjaccardJoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamazonCols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgoogleCols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresultDF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mgroundTruth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Amazon_Google_perfectMapping_sample\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m                           \u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midAmazon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midGoogle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1298\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1299\u001b[0m             raise AttributeError(\n\u001b[1;32m-> 1300\u001b[1;33m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0m\u001b[0;32m   1301\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1302\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    source_path = \"data/amazon-google-sample/\"\n",
    "    er = EntityResolution(source_path+\"Amazon_sample\", source_path+\"Google_sample\", source_path+\"stopwords.txt\")\n",
    "    amazonCols = [\"title\", \"manufacturer\"]\n",
    "    googleCols = [\"name\", \"manufacturer\"]\n",
    "    resultDF = er.jaccardJoin(amazonCols, googleCols, 0.5)\n",
    "    result = resultDF.rdd.map(lambda row: (row.id1, row.id2)).collect()\n",
    "    groundTruth = spark.read.parquet(\"data/amazon-google-sample/Amazon_Google_perfectMapping_sample\"). \\\n",
    "                          rdd.map(lambda row: (row.idAmazon, row.idGoogle)).collect()\n",
    "    print (\"(precision, recall, fmeasure) = \", er.evaluate(result, groundTruth))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
